{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "76d1bad6-5e12-4284-b339-f79841c37f8a",
      "metadata": {
        "id": "76d1bad6-5e12-4284-b339-f79841c37f8a"
      },
      "source": [
        "# **Phase-3** :\n",
        "**--Transforming raw insurance claim data using feature engineering, building and evaluating predictive models, and deploying**\n",
        "\n",
        "# **Feature Engineering :**\n",
        "\n",
        "\n",
        "-Date Parsing: Converted insurance_apply_date and insurance_claimed_date from string to datetime formats to enable calculation of elapsed time between policy application and claim.\n",
        "-Handling Missing Values: Removed records containing missing values to ensure model input quality and integrity.\n",
        "-Categorical Encoding: Encoded categorical variables (region, sex, smoker) into numerical formats using LabelEncoder, making them machine-readable for modeling.\n",
        "-Derived Feature Creation: Engineered a new feature, claim_delay, quantifying the number of days between policy application and claim—potentially informative for identifying delays or fraud.\n",
        "-Outlier Treatment: Used z-score filtering to detect and remove records where numeric features (age, children, bmi, bill_amount, claimed_amount, amount_paid, duration) had unusually high or low values, thus minimizing model distortion from anomalies.\n",
        "-Dropping Irrelevant Features: Removed non-informative columns such as identifiers (patient_id, full_name, region_code) and raw date columns, focusing only on predictive features for model training.\n",
        "-Numerical Scaling: Standardized key numeric variables using StandardScaler to ensure all features contributed proportionally during model training, improving convergence and performance of most algorithms.\n",
        "-Final Feature Matrix: After processing, the feature set included only relevant, encoded, and scaled attributes with strong predictive potential for insurance claim classification and fraud detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07f34172-e341-4038-b89d-7db83f787a47",
      "metadata": {
        "id": "07f34172-e341-4038-b89d-7db83f787a47"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('C:\\\\Users\\\\RENUKA\\\\Downloads\\\\nhic_data.csv')\n",
        "\n",
        "# 1. Convert date columns to datetime\n",
        "df['insurance_apply_date'] = pd.to_datetime(df['insurance_apply_date'])\n",
        "df['insurance_claimed_date'] = pd.to_datetime(df['insurance_claimed_date'])\n",
        "\n",
        "# 2. Handle missing values (if any)\n",
        "df = df.dropna()  # or you can impute, depending on context\n",
        "\n",
        "# 3. Encode categorical variables\n",
        "lbl_region = LabelEncoder()\n",
        "lbl_sex = LabelEncoder()\n",
        "lbl_smoker = LabelEncoder()\n",
        "df['region'] = lbl_region.fit_transform(df['region'])\n",
        "df['sex'] = lbl_sex.fit_transform(df['sex'])\n",
        "df['smoker'] = lbl_smoker.fit_transform(df['smoker'])\n",
        "\n",
        "# 4. Create derived features\n",
        "df['claim_delay'] = (df['insurance_claimed_date'] - df['insurance_apply_date']).dt.days\n",
        "\n",
        "# 5. Detect and treat outliers in numerical columns with Z-score\n",
        "numerical_cols = ['age', 'children', 'bmi', 'bill_amount', 'claimed_amount', 'amount_paid', 'duration']\n",
        "zscores = np.abs((df[numerical_cols] - df[numerical_cols].mean()) / df[numerical_cols].std())\n",
        "df = df[(zscores < 3).all(axis=1)]\n",
        "\n",
        "# 6. Drop irrelevant columns\n",
        "df = df.drop(['patient_id', 'full_name', 'region_code', 'insurance_apply_date', 'insurance_claimed_date'], axis=1)\n",
        "\n",
        "# 7. Normalize/Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "num_feature_cols = ['age', 'children', 'bmi', 'bill_amount', 'claimed_amount', 'amount_paid', 'duration', 'claim_delay']\n",
        "df[num_feature_cols] = scaler.fit_transform(df[num_feature_cols])\n",
        "\n",
        "# 8. Ready for model training\n",
        "X = df.drop(['insuranceclaim'], axis=1)\n",
        "y = df['insuranceclaim']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7c99344-750c-4d2a-8a8d-8ceadc40d0d0",
      "metadata": {
        "id": "b7c99344-750c-4d2a-8a8d-8ceadc40d0d0"
      },
      "source": [
        "# **Column Name Check and Cleanup**:\n",
        "\n",
        "-Used print(df.columns.tolist()) to display the current column names in the DataFrame and diagnose issues with spaces or typos from source files.\n",
        "-Applied df.columns = df.columns.str.strip() to remove any leading or trailing whitespace in column names, ensuring all column references in code are correct, consistent, and free from formatting errors.\n",
        "-This step is especially important after reading in externally sourced CSV files, as even minor inconsistencies or hidden spaces can cause KeyErrors when selecting or transforming columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dec816fc-af16-45f4-a795-efdbbf2dcc7e",
      "metadata": {
        "id": "dec816fc-af16-45f4-a795-efdbbf2dcc7e",
        "outputId": "38d149fa-8601-4836-965e-3a47cefb2557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['age', 'children', 'sex', 'region', 'bmi', 'smoker', 'bill_amount', 'insuranceclaim', 'claimed_amount', 'amount_paid', 'duration', 'year_billing', 'claim_delay']\n"
          ]
        }
      ],
      "source": [
        "print(df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "532813a4-59ff-4b3c-9c26-1082e767cbb8",
      "metadata": {
        "id": "532813a4-59ff-4b3c-9c26-1082e767cbb8"
      },
      "outputs": [],
      "source": [
        "df.columns = df.columns.str.strip()  # Ex: \" insuranceclaim \"  ----->   \"insuranceclaim\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "929d367e-5fba-499e-b14a-3b09098d7581",
      "metadata": {
        "id": "929d367e-5fba-499e-b14a-3b09098d7581"
      },
      "source": [
        "# **Model Training and Selection :**\n",
        "\n",
        "-Data Preparation: After feature engineering, split the processed data into features (X) and target labels (y), followed by a train-test split to objectively evaluate model performance.\n",
        "-Scaling: Standardized feature values using StandardScaler to ensure uniform input ranges for all models, promoting fair comparison and optimal training for algorithms sensitive to feature scale.\n",
        "-Model Comparison: Trained three classification algorithms—Random Forest, XGBoost, and Logistic Regression—on the standardized training data. Evaluated each using key metrics: accuracy, precision, recall, and F1 score.\n",
        "-Best Model Selection: Computed all metrics on the test set. The model achieving the highest F1 Score (balancing precision and recall) was selected for deployment.\n",
        "-Serialization: Saved the best-performing model and the scaler together in a .pkl file using Python’s pickle module. This ensures deployment in other environments, including Streamlit, without retraining or re-scaling steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9320999f-c853-4547-ab39-bda56f4c4381",
      "metadata": {
        "id": "9320999f-c853-4547-ab39-bda56f4c4381",
        "outputId": "76c1b64b-ecf1-4b1b-e976-1799f615a540"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\RENUKA\\anaconda3\\jupyter\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [19:06:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pickle\n",
        "\n",
        "# Load data (with correct column names)\n",
        "df = pd.read_csv('C:\\\\Users\\\\RENUKA\\\\Downloads\\\\nhic_data.csv')\n",
        "df.columns = df.columns.str.strip()  # Clean column names\n",
        "\n",
        "# Convert date columns to datetime\n",
        "df['insurance_apply_date'] = pd.to_datetime(df['insurance_apply_date'])\n",
        "df['insurance_claimed_date'] = pd.to_datetime(df['insurance_claimed_date'])\n",
        "\n",
        "# Feature engineering\n",
        "df['claim_delay'] = (df['insurance_claimed_date'] - df['insurance_apply_date']).dt.days\n",
        "for col in ['sex', 'region', 'smoker']:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# Prepare feature matrix and target\n",
        "X = df.drop(['insuranceclaim', 'patient_id', 'full_name', 'insurance_apply_date', 'insurance_claimed_date', 'region_code'], axis=1)\n",
        "y = df['insuranceclaim']\n",
        "\n",
        "# Train/test split and scaling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Make sure xgboost is installed:\n",
        "# pip install xgboost OR conda install -c conda-forge xgboost\n",
        "\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    results[name] = {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1 Score\": f1_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "best_model_name = max(results, key=lambda k: results[k]['F1 Score'])\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "with open('final_model.pkl', 'wb') as f:\n",
        "    pickle.dump((best_model, scaler), f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eb8b628-dcb6-45f5-814d-56c607e3571e",
      "metadata": {
        "id": "9eb8b628-dcb6-45f5-814d-56c607e3571e",
        "outputId": "790d1e6d-0d4e-425f-ab66-5ca3bf627adf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in c:\\users\\renuka\\anaconda3\\jupyter\\lib\\site-packages (3.0.5)\n",
            "Requirement already satisfied: numpy in c:\\users\\renuka\\anaconda3\\jupyter\\lib\\site-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: scipy in c:\\users\\renuka\\anaconda3\\jupyter\\lib\\site-packages (from xgboost) (1.13.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f54420cf-6fe7-4b4a-be10-919fdbfeebcf",
      "metadata": {
        "id": "f54420cf-6fe7-4b4a-be10-919fdbfeebcf"
      },
      "source": [
        "# **Model Performance Metrics Table:**\n",
        "-Compiled the evaluation metrics (Accuracy, Precision, Recall, and F1 Score) for each trained classification model (Random Forest, XGBoost, and Logistic Regression) into a pandas DataFrame for easy comparison.\n",
        "-Displayed the metrics in tabular format to visually assess which model performed best on the test set, supporting informed selection for deployment.\n",
        "-This step helps interpret model strengths, weaknesses, and trade-offs between different algorithms and is critical for transparent reporting in machine learning projects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "782dfa6a-8112-4074-a655-ef436e29534c",
      "metadata": {
        "id": "782dfa6a-8112-4074-a655-ef436e29534c",
        "outputId": "919df4a4-73bb-4db4-ecbb-568714704be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     Accuracy  Precision  Recall  F1 Score\n",
            "Random Forest        1.000000   1.000000     1.0  1.000000\n",
            "XGBoost              0.996269   0.993711     1.0  0.996845\n",
            "Logistic Regression  1.000000   1.000000     1.0  1.000000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c38ca7c9-2864-4956-8e1e-da5007bec9bb",
      "metadata": {
        "id": "c38ca7c9-2864-4956-8e1e-da5007bec9bb"
      },
      "source": [
        "# **Best Model Identification and Metrics:**\n",
        "\n",
        "\n",
        "-Programmatically determined the best-performing classifier by selecting the model with the highest F1 score from all evaluated machine learning algorithms.\n",
        "-Printed both the model’s name and its complete set of test metrics (accuracy, precision, recall, F1) for easy reference and clear justification of the model selection process.\n",
        "-These outputs provide transparency in the workflow and support robust, data-driven decision-making for deployment in production or application settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ae6af56-f3ba-4e64-83a1-55418262d30d",
      "metadata": {
        "id": "3ae6af56-f3ba-4e64-83a1-55418262d30d",
        "outputId": "beebe310-b1f3-44e5-c8c9-b50154d40059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model: Random Forest\n",
            "Best Model Metrics: {'Accuracy': 1.0, 'Precision': 1.0, 'Recall': 1.0, 'F1 Score': 1.0}\n"
          ]
        }
      ],
      "source": [
        "print(\"Best Model:\", best_model_name)\n",
        "print(\"Best Model Metrics:\", results[best_model_name])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7dc34c4-f543-4916-a4c0-32ea0dee89fd",
      "metadata": {
        "id": "f7dc34c4-f543-4916-a4c0-32ea0dee89fd"
      },
      "source": [
        "# **Model and Scaler Loading for Deployment:**\n",
        "\n",
        "\n",
        "-Loaded the best-performing trained model and the associated StandardScaler from the serialized final_model.pkl file using Python’s pickle library.\n",
        "-This step ensures that both the model and the scaler can be reused for future predictions and deployments (e.g., Streamlit app) without retraining or re-scaling, maintaining workflow consistency and reproducibility.\n",
        "-Printing the loaded model confirms that the correct classifier is available and ready for inference on new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcd37c2e-98bd-4944-9e41-96915ab63ce5",
      "metadata": {
        "id": "fcd37c2e-98bd-4944-9e41-96915ab63ce5",
        "outputId": "6b2ee13f-7c27-4b70-8595-e6eb14a8fb2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForestClassifier(random_state=42)\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "with open('final_model.pkl', 'rb') as f:\n",
        "    model, scaler = pickle.load(f)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de58485e-387c-4076-a3ce-d10df8deaf06",
      "metadata": {
        "id": "de58485e-387c-4076-a3ce-d10df8deaf06"
      },
      "source": [
        "# **Feature List Confirmation:**\n",
        "\n",
        "\n",
        "-Displayed the full list of input (feature) columns used for model training after preprocessing and feature engineering.\n",
        "-This printout serves as a reference to ensure all necessary features are included, in the correct order, for reproducible training, evaluation, and deployment (including Streamlit integration).\n",
        "-Allows quick verification that non-predictive, identifier, or dropped columns are excluded, and that derived variables (like claim_delay) are present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b9f28e0-10e8-4551-bb10-f3ea4188533b",
      "metadata": {
        "id": "1b9f28e0-10e8-4551-bb10-f3ea4188533b",
        "outputId": "39bd9462-6f3d-4c76-ec86-33dbfafc2255"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['age', 'children', 'sex', 'region', 'bmi', 'smoker', 'bill_amount', 'claimed_amount', 'amount_paid', 'duration', 'year_billing', 'claim_delay']\n"
          ]
        }
      ],
      "source": [
        "print(X.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea30317c-6f20-494b-8354-b95c37854e9a",
      "metadata": {
        "id": "ea30317c-6f20-494b-8354-b95c37854e9a",
        "outputId": "ac38b1f3-600f-4c3c-d61e-9848f2ba0378"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        age  children       sex    region       bmi    smoker  bill_amount  \\\n",
            "0  0.568178  1.603053  1.007505 -1.350761 -0.142996  1.976931     1.006389   \n",
            "1 -1.137352 -0.892485 -0.992551  1.350761 -0.417860 -0.505835    -0.871717   \n",
            "2  0.994560  1.603053 -0.992551 -1.350761 -0.421132 -0.505835    -0.120153   \n",
            "3  1.065624  1.603053 -0.992551  1.350761 -1.505864 -0.505835    -0.059240   \n",
            "4  0.639241 -0.060639  1.007505  1.350761  0.981003 -0.505835     1.268720   \n",
            "\n",
            "   claimed_amount  amount_paid  duration  year_billing  claim_delay  \\\n",
            "0        1.170329    -0.411173 -0.754181     -0.333887    -0.754181   \n",
            "1       -0.515613    -0.726613 -1.199331     -0.333887    -1.199331   \n",
            "2        0.175844    -0.636885  0.084357      0.862960     0.084357   \n",
            "3       -0.711943     1.422050 -0.236565      1.461383    -0.236565   \n",
            "4        1.431031    -0.421921 -0.971580      0.264537    -0.971580   \n",
            "\n",
            "   fraud_probability  actual_claim  \n",
            "0               1.00             1  \n",
            "1               1.00             1  \n",
            "2               1.00             1  \n",
            "3               0.00             0  \n",
            "4               0.97             1  \n"
          ]
        }
      ],
      "source": [
        "# Assume X_test is your test feature matrix\n",
        "fraud_prob = best_model.predict_proba(X_test)[:, 1]  # probability of class '1' (fraudulent claim)\n",
        "\n",
        "# Add to your DataFrame (for test set)\n",
        "results_df = pd.DataFrame(X_test, columns=X.columns)  # or use pd.DataFrame(scaled values, ...)\n",
        "results_df['fraud_probability'] = fraud_prob\n",
        "results_df['actual_claim'] = y_test.values  # Optional: add actual class for comparison\n",
        "\n",
        "# Save or inspect\n",
        "print(results_df.head())\n",
        "# Optionally, save to CSV:\n",
        "results_df.to_csv(\"insurance_test_with_fraud_prob.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "328968db-ee40-44f8-aeac-06096c47d611",
      "metadata": {
        "id": "328968db-ee40-44f8-aeac-06096c47d611"
      },
      "source": [
        "\n",
        "\n",
        "# **Probability Score and Data Export**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b63ffb3-39a3-4de8-8e2f-14d9c08da094",
      "metadata": {
        "id": "5b63ffb3-39a3-4de8-8e2f-14d9c08da094",
        "outputId": "214c8d4f-7f56-4dad-84bb-1c8d7cae525d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   age  children  sex  region     bmi  smoker  bill_amount  claimed_amount  \\\n",
            "0   35         2    0       2  38.095       0     24915.05            0.00   \n",
            "1   18         0    1       2  28.500       0      1712.23         1606.95   \n",
            "2   48         0    0       0  28.900       0      8277.52         7547.65   \n",
            "3   18         0    1       1  53.130       0      1163.46         1054.89   \n",
            "4   22         1    1       1  52.580       1     44501.40        40097.64   \n",
            "\n",
            "   amount_paid  duration  year_billing  claim_delay  fraud_probability  \\\n",
            "0     24915.05       362          2019          362               0.00   \n",
            "1       105.28       254          2020          254               1.00   \n",
            "2       729.87       187          2022          187               1.00   \n",
            "3       108.57       181          2022          181               1.00   \n",
            "4      4403.76       158          2023          158               0.98   \n",
            "\n",
            "   actual_claim  \n",
            "0             0  \n",
            "1             1  \n",
            "2             1  \n",
            "3             1  \n",
            "4             1  \n"
          ]
        }
      ],
      "source": [
        "# Step 1: Prepare your full feature matrix (exclude target and identifiers)\n",
        "X_full = df.drop(['insuranceclaim', 'patient_id', 'full_name', 'insurance_apply_date', 'insurance_claimed_date', 'region_code'], axis=1)\n",
        "\n",
        "# Step 2: Scale the features using the trained scaler\n",
        "X_full_scaled = scaler.transform(X_full)\n",
        "\n",
        "# Step 3: Predict fraud probability for each record\n",
        "fraud_prob = best_model.predict_proba(X_full_scaled)[:, 1]  # Probability of class '1' (fraudulent claim)\n",
        "\n",
        "# Step 4: Construct output dataframe\n",
        "results_full = X_full.copy()\n",
        "results_full['fraud_probability'] = fraud_prob\n",
        "results_full['actual_claim'] = df['insuranceclaim'].values\n",
        "\n",
        "# Step 5: Save to CSV\n",
        "results_full.to_csv(\"insurance_fraud_prob_full.csv\", index=False)\n",
        "\n",
        "print(results_full.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf078611-c60f-4454-b972-8a6c455784c6",
      "metadata": {
        "id": "bf078611-c60f-4454-b972-8a6c455784c6"
      },
      "outputs": [],
      "source": [
        "results_full.to_csv(\"insurance_fraud_prob_full.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}